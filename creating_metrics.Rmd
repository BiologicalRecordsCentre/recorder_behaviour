---
title: "Metrics for Recorder behaviour"
author: "Tom August"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```
```{r libraries, echo = FALSE, message = FALSE}
library(sp)
library(BRCmap)
library(adehabitatHR)
library(raster)
library(Deducer)
```
```{r data_cleaning, echo = FALSE}
# # There are a range of data cleaning steps that I will probably need to do
# # I can put them in here
# iRB <- read.csv('data/iRecord Butterflies 2016 10 11.csv',
#                 stringsAsFactors = FALSE)
# 
# # Make the date column a date
# iRB$date_start <- as.Date(as.character(iRB$date_start))
# 
# # App started in 2014 so crop data to then
# iRB <- iRB[iRB$date_start >= "2014-01-01", ]
# 
# # Only keep the butterflies
# iRB <- iRB[iRB$taxon_group == 'insect - butterfly', ]
# 
# # I need to mask the butterfly data (and other datasets)
# # to the UK as they are a bit all over the place
# 
# # Convert to SpatialPoints
# spPoints_LL <- SpatialPointsDataFrame(coords = iRB[ , c('long', 'lat')],
#                                       data = iRB[ , colnames(iRB)[!colnames(iRB) %in% c('long', 'lat')]])
# 
# # Data is lat long
# proj4string(spPoints_LL) <- CRS("+init=epsg:4326")
# 
# # Get a polygon for the UK boundary
# IoM <- readRDS('data/country shapefiles/IMN_adm0.rds')
# GBR <- readRDS('data/country shapefiles/GBR_adm0.rds')
# 
# # Adjust rownames so they dont crash between files
# IoM <- spChFIDs(IoM, paste("IoM", row.names(IoM), sep="."))
# 
# # Bind the 2 together and transform to the same CRS as
# # the point data
# UK <- spTransform(rbind(IoM, GBR), CRSobj = spPoints_LL@proj4string)
# 
# # Subset points to the UK polygon
# UKpoints <- spPoints_LL[UK, ]
# 
# # Convert to a data.frame
# UKpointsDF <- as.data.frame(UKpoints)
# 
# # save for future use
# save(UKpointsDF, file = 'data/iRB_UK.rdata')

# load in cleaned data saved above
load('data/iRB_UK.rdata')
iRB <- UKpointsDF
rm(list = c('UKpointsDF'))
```

## Metrics

We are going to split metric into three broad groups: *Engagement profile*, *Spatial*, and *Taxanomic*

## Temporal Metrics

These metrics measure the recording pattern across time such as the number of days that a recorder produces records. These have been termed engagement profiles by others., The metrics here are from Ponciano and Brasileiro 2014 who used the metrics on participant of zooniverse projects. The metrics were also used by Boakes *et al* 2016.

### Summer period

One issue that we have across these metrics and some others is that recording is not consistent across the year and so there can be issues with the numbers generated. To address this the data can be subset to only the summer period, when recorders are active. This period needs to be defined in such a way that the same method can be used across taxanomic groups and will be robust to changes in the start and end of teh summer period from year to year.

I suggest we use a percentage cut off, for example take the period of the year that contains 90% of the data. Lets have a look at how this might work

```{r summer_period}
# First lets have a look at the distribution of 
# records through the year
jday <- as.POSIXlt(iRB$date_start)$yday

# Where are the limits of different percentiles
p95 <- quantile(jday, c(0.025,0.975))
p90 <- quantile(jday, c(0.05,0.95))
p80 <- quantile(jday, c(0.1,0.9))
p70 <- quantile(jday, c(0.15,0.85))

hist(jday, 100, main = paste('Histogram of recording day with\n',
                             'cutoffs at 95%, 90%, 80%, & 70%'))
abline(v = p95, lty = 1, col = 'darkred', lwd = 2)
abline(v = p90, lty = 5, col = 'darkred', lwd = 2)
abline(v = p80, lty = 2, col = 'darkred', lwd = 2)
abline(v = p70, lty = 3, col = 'darkred', lwd = 2)

```

It looks like 90% might be a good value to go for in this case. We would then need a function that could create these values for each year of the data and throw out data that was outside the summer periods

```{r summer_function}
summerData <- function(data, probs = c(0.05, 0.95),
                       date_col = 'date_start'){
  
  # check date column
  if(!inherits(data[, date_col], 'Date')){
    stop('Your date column is not a date')
  }
  
  # create J-day column
  data$Jday <- as.POSIXlt(data[,date_col])$yday
  
  # create year column
  data$year <- as.POSIXlt(data[,date_col])$year+1900
  
  # create summer column
  data$summer <- FALSE
  
  qsf <- qsl <- NULL  
  
  # now for each year loop through and create an 
  # index column
  for(i in sort(unique(data$year))){
    
    year_quantiles <- quantile(data$Jday[data$year == i], probs = probs)
    qsf <- c(qsf, year_quantiles[1])
    qsl <- c(qsl, year_quantiles[2])
    data$summer[data$Jday >= year_quantiles[1]
                & data$Jday <= year_quantiles[2] 
                & data$year == i] <- TRUE
  }
  
  summer_data <- data[data$summer, ]
  attr(summer_data, 'cutoffs') <- data.frame(year = sort(unique(data$year)),
                                             quantile_first = qsf,
                                             quantile_last = qsl)  
  return(summer_data)
}

summer_iRB <- summerData(data = iRB,
                         probs = c(0.05, 0.95),
                         date_col = 'date_start')

# Look at the 'cut' data 
hist(summer_iRB$Jday, 100)

# Here are the cuts
attr(summer_iRB, 'cutoffs')

```

### Activity ratio

*"The proportion of days on which the volunteer was active in relation to the total days he/she remained linked to the project"* (Ponciano and Brasileiro 2014)

```{r activity_ratio}
# Create a function to calculate activity ratio
activityRatio <- function(recorder_name,
                          data,
                          recorder_col = 'recorders',
                          date_col = 'date_start'){
  
  # check date column
  if(!inherits(data[, date_col], 'Date')){
    stop('Your date column is not a date')
  }
  
  # Get the recorders data
  data <- data[data[,recorder_col] == recorder_name, ]
  
  # Some people might have no data from the summer period
  if(nrow(data) < 1){
    
    return(data.frame(recorder = recorder_name,
                      activity_ratio = NA,
                      total_duration = NA,
                      active_days = NA))
  } else {
    
  # Get unique dates
  dates <- unique(data[,date_col])
  
  # Get the first and last date
  first_last <- range(dates)
  
  # Total duration of this recorder
  duration <- as.numeric(first_last[2] - first_last[1]) + 1 
  
  # calculate ratio
  activity_ratio <- length(dates)/duration
  
  # return
  return(data.frame(recorder = recorder_name,
                    activity_ratio = activity_ratio,
                    total_duration = duration,
                    active_days = length(dates)))
  }
}

# Test on David and Tom
activityRatio(data = summer_iRB, recorder_name = 'Roy, David')
activityRatio(data = summer_iRB, recorder_name = 'August, Tom')

## David is a more active recorder than Tom ##

# Run for everyone
all_AR <- do.call(rbind, lapply(X = unique(iRB$recorders),
                                FUN = activityRatio,
                                data = summer_iRB))

# Lets have a look at some of these
head(all_AR, 20)
```

I think this metric tells a story in a combination of the ratio and the total number of days. I think the ratio means more when the recorder has been recording for a long duration

```{r plot_activity_ratio1}
# Have a look at the distribution of these 2 metrics
# There looks like there could be an effect of year
hist(all_AR$total_duration, breaks = 30)
hist(all_AR$activity_ratio)
```

Both have nice distributions, though we can see the single record people in the ratio plot

```{r plot_activity_ratio2}
# Plot activity_ratio against duration
# Here we probably want to subset to avoid bias
plot(all_AR$total_duration,
     all_AR$activity_ratio)

plot(all_AR$total_duration[all_AR$total_duration >= 10],
     all_AR$activity_ratio[all_AR$total_duration >= 10])
```

### Weekly devoted days

This is an adaptation of the *Daily Devoted Time* in (Ponciano and Brasileiro 2014) which is clearly not applicable to biological recording. Though Boakes *et al* 2016 don't attempt to use this measure I think the idea can be adapted by using days in a week (summer only) rather than hours in a day.

```{r weekly_devoted_days}
# Create a function
weeklyDevotedDays <- function(recorder_name,
                              data,
                              recorder_col = 'recorders',
                              date_col = 'date_start'){
  
  # check date column
  if(!inherits(data[, date_col], 'Date')){
    stop('Your date column is not a date')
  }
  
  # Get the recorders data
  data <- data[data[,recorder_col] == recorder_name, ]
  
  # Get unique dates as dates
  dates <- unique(data[,date_col])
  
  # Get all week_year combinations
  week_year <- paste(strftime(as.POSIXlt(dates), format = '%W'),
                     format(dates, '%Y'), sep = '_')
  
  # here are the counts
  week_counts <- table(week_year)
  
  # As these are counts taking the median is probably best
  weekly_devoted_days <- median(week_counts)
  
  return(data.frame(recorder = recorder_name,
                    median_weekly_devoted_days = weekly_devoted_days,
                    n_weeks = length(week_counts),
                    n_recs = sum(week_counts), row.names = NULL))
}

# Test on David and Tom
weeklyDevotedDays(data = summer_iRB, recorder_name = 'Roy, David')
weeklyDevotedDays(data = summer_iRB, recorder_name = 'August, Tom')

## David contributes more of his time than Tom ##

# Run for everyone
all_WDD <- do.call(rbind, lapply(X = unique(iRB$recorders),
                                 FUN = weeklyDevotedDays,
                                 data = summer_iRB))

# Lets have a look at some of these
head(all_WDD, 20)
```

Clearly this metric is only really reliable when we have multiple weeks worth of data for an individual.

### Relative activity duration

This is a metric from Ponciano and Brasileiro 2014 which is also used in Boakes *et al* 2016 but I don't think can be applied to biological records since there is no official end date for a project: *"The ratio of days during which a volunteer I remains linked to the project in relation to the total number of days elapsed since the volunteer joined the project until the project is over"*

### Periodicity

There is a cluster of metrics that could be used to look at aspects of periodicity. The measure used in Ponciano and Brasileiro 2014 is 'variation in periodicity'; *"The standard deviation of the times elapsed between each pair of sequential active days"*. At the same time as calculating this I think there are another couple of metrics that might be of use. First, periodicity itself, i.e. *"The median time elapsed between each pair of sequential active days"*. Secondly, streak length, i.e. *"The average length of sequential active days"*

```{r periodicity}
# Create a function to calculate the periodicity metrics
periodicity <- function(recorder_name,
                        data,
                        recorder_col = 'recorders',
                        date_col = 'date_start',
                        day_limit = 5){
  
  # check date column
  if(!inherits(data[, date_col], 'Date')){
    stop('Your date column is not a date')
  }
  
  # Get the recorders data
  data <- data[data[,recorder_col] == recorder_name, ]
  
  # Get unique dates as dates
  dates <- sort(unique(data[,date_col]))
  
  # we cannot calculate these metrics if people have very few
  # dates on which they record
  if(length(unique(dates)) < day_limit){
    
    # return
    return(data.frame(recorder = recorder_name,
                      periodicity = NA,
                      periodicity_variation = NA,
                      median_streak = NA,
                      sd_streak = NA,
                      max_streak = NA,
                      n_days = length(unique(dates))))
      
  } else {
    
    # Calculate the elapsed days between each date in sequence
    # this needs to be done within years
    elapses <- NULL
    
    for(year in unique(format(dates, '%Y'))){
      
      temp_dates <- dates[format(dates, '%Y') == year]
       
      # There must be at least 2 dates in a year
      if(length(temp_dates) > 1){
        temp_elapses <- sapply(1:(length(temp_dates)-1),
           FUN = function(x){
             return(as.numeric(temp_dates[x + 1] - temp_dates[x]))
           })  
      
        elapses <- c(elapses, temp_elapses)
      
      }
      
    }
  
    # periodicity calculation
    periodicity <- median(elapses)
    
    # variation in periodicity
    periodicity_variation <- sd(elapses)
    
    # average streak length
    # Streaks are IDed by 1's
    non_streak <- length(elapses[elapses > 1])
    streaks <- rle(elapses)
    streaks_1 <- (streaks$lengths[streaks$value == 1]) + 1
    
    # Combine streaks and non-streaks
    streak_lengths <- c(rep(1, non_streak), streaks_1)
    
    # calculate ome metrics
    median_streak <- median(streak_lengths)
    sd_streak <- sd(streak_lengths)
    max_streak <- max(streak_lengths)
    
    # return
    return(data.frame(recorder = recorder_name,
                      periodicity = periodicity,
                      periodicity_variation = periodicity_variation,
                      median_streak = median_streak,
                      sd_streak = sd_streak,
                      max_streak = max_streak,
                      n_days = length(unique(dates))))
    
  }
  
  
}

# Test on David and Tom
periodicity(data = summer_iRB, recorder_name = 'Roy, David')
periodicity(data = summer_iRB, recorder_name = 'August, Tom')

# David is a much more regular recorder than Tom with less
# variation in periodicity and a longer max streak though
# Tom has less days of data to work with

# Run for everyone
all_P <- do.call(rbind, lapply(X = unique(iRB$recorders),
                               FUN = periodicity,
                               data = iRB))

# Lets have a look at some of these
head(all_P, 20)[c(5,8,1),]

# David a Tam are both very studious recorders with
# long max streaks and very low periodicity. 
# Anne is less studious but still has a low periodicity

# Nice poission dist. for periodicity
hist(all_P$periodicity[all_P$periodicity < 50],
     breaks = 50)

# Nice poission dist. for periodicity_variation (long tail)
hist(all_P$periodicity_variation[all_P$periodicity_variation < 200],
     breaks = 100)

# Dist. of max_streak
hist(all_P$max_streak[all_P$max_streak < 20],
     breaks = 20)
```

By using the summer data only this analysis seems to be better than an earlier one that included all data. These metrics cannot be calculate for people who have only made one record. I have included a parameter `day_limit` to allow us to set a limit at which we calculate these metrics.


## Spatial Meterics

These metrics deal with the spatial distribution of records

### Area and heterogenity of recording

I think the first step for all of these metrics is to turn the  points into a SpatialPoints object which will allow us to manipulate then more easily. Once we have done that we can calculate MCP (minimum convex polygons) around the points. We might want to change this method to a method that is less susceptible to outliers such as alpha hull (we can talk to Colin about this). Here I use 95% MCP as the total recording area (hopefully removing outliers), and use the ratio of 95%:50% as a measure of heterogeneity.


```{r spatialpoint}
# Function takes data and username and returns spatial metrics
spatial_behaviour <- function(data, recorder_name,
                              latitude_col, longitude_col,
                              recorder_col = 'recorders',
                              upper_percentile = 95,
                              lower_percentile = 60,
                              h = 5000,
                              res = 1000){
  
  if(is.factor(recorder_name)){
    recorder_name <- as.character(recorder_name)
  } 
  
  n_row <- nrow(iRB[iRB[,recorder_col] == recorder_name, ])
  
  if(n_row >= 5){
  
    # Convert to SpatialPoints
    spPoints_LL <- SpatialPoints(iRB[iRB[,recorder_col] == recorder_name,
                                     c(longitude_col, latitude_col)])
    # Data is lat long
    proj4string(spPoints_LL) <- CRS("+init=epsg:4326")
  
    # Convert to Eastings Northings to get meters on X and Y
    spPoint_UK <- spTransform(spPoints_LL, "+init=epsg:27700")
    
    # set up grid
    # This allows us to ensure there is space for the 
    # isoclines to be drawn and that the pixel res is the
    # same - here 1km
    minlong <- floor(bbox(spPoint_UK)['long','min'] - (10*h))
    minlat <- floor(bbox(spPoint_UK)['lat','min'] - (10*h))
    maxlong <- ceiling(bbox(spPoint_UK)['long','max'] + (10*h))
    maxlat <- ceiling(bbox(spPoint_UK)['lat','max'] + (10*h))

    grid_ras <- raster(ext = extent(minlong, maxlong, minlat, maxlat),
                       res = res,
                       crs = projection(spPoint_UK))
                       
                       # matrix(NA,
                       # ncol = ceiling(nlat),
                       # nrow = ceiling(nlong)))
    
    grid_SP <- as(grid_ras, "SpatialPixels")
    
    # Try kernel density
    KD <- kernelUD(xy = spPoint_UK, h = h, grid = grid_SP)
    # image(KD)
    KA <- kernel.area(KD,
                percent = c(lower_percentile, upper_percentile),
                unin = "m",
                unout = "km2")
    area_upper <- KA[2]
    area_lower <- KA[1]
    poly_lower <- getverticeshr(KD, percent = lower_percentile)
    poly_upper <- getverticeshr(KD, percent = upper_percentile)
    rm(list = 'KD')
    npolys_upper <- length(poly_upper@polygons[[1]]@Polygons)
    npolys_lower <- length(poly_lower@polygons[[1]]@Polygons)

    # # Calculate the Local convex hull
    # LCH_poly <- LoCoH.k(xy = spPoint_UK,
    #                     k =  10,
    #                     unin = 'm',
    #                     unout = 'km',
    #                     duplicates = 'remove')
    # 
    # # Extract percentiles
    # LCH_MCHu <- spoldf2MCHu(LCH_poly)
    # poly_upper <- getverticeshr.MCHu(LCH_MCHu,
    #                                  percent = upper_percentile)
    # poly_lower <- getverticeshr.MCHu(LCH_MCHu,
    #                                  percent = lower_percentile)
    # 
    # npolys_upper <- length(poly_upper@polygons[[1]]@Polygons)
    # npolys_lower <- length(poly_lower@polygons[[1]]@Polygons)
    # 
    # area_upper <- MCHu2hrsize(x = LCH_MCHu, percent = upper_percentile,
    #                           plotit = FALSE)
    # area_lower <- MCHu2hrsize(x = LCH_MCHu, percent = lower_percentile,
    #                           plotit = FALSE)

    return(list(recorder = recorder_name,
                spPoint_UK = spPoint_UK,
                # poly = LCH_poly,
                poly_upper = poly_upper,
                poly_lower = poly_lower,
                upper_n_poly = npolys_upper,
                lower_n_poly = npolys_lower,
                upper_area = area_upper,
                lower_area = area_lower,
                ratio = area_lower/area_upper,
                n = n_row))
  } else {
    return(list(recorder = recorder_name,
                spPoint_UK = NA,
                # poly = NA,
                poly_upper = NA,
                poly_lower = NA,
                upper_n_poly = NA,
                lower_n_poly = NA,
                upper_area = NA,
                lower_area = NA,
                ratio = NA,
                n = n_row))
  }
 
}

# Function for plotting records
plot_ratio <- function(data){
  om <- par("mar")
  omf <- par('mfrow')
  par(mfrow = c(1,2), mar = c(1,1,1,1))
  data(UK)
  plot_GIS(UK, new.window = FALSE,
           main = 'Distribution of records',
           show.axis = FALSE, show.grid = FALSE)
  points(data$spPoint_UK, pch = 3, col = 'blue')
  
  # Plot heat map
  plot(data$poly_upper,
       main = paste('\n\n', data$recorder, '-', 'Ratio:',
                    round(data$ratio, 4), '\n',
                    'Upper/lower polygons:', data$upper_n_poly,
                    '/', data$lower_n_poly, '\n',
                    'Total Area:', data$upper_area),
       col = 'grey')
  plot(data$poly_lower, add = TRUE,
       col = 'red', border = 'red')
  points(data$spPoint_UK, col = rgb(0,0,0,0.4),
         pch = 3)
  par(mfrow = omf,
      mar = om)
}

for(h in c(1000, 5000, 10000)){
  DD <- spatial_behaviour(data = iRB, recorder_name = 'Roy, David',
                          latitude_col = 'lat', longitude_col = 'long',
                          h = h)
  plot_ratio(data = DD)
  
}

# for(recorder in c('Partridge, Francesca', 'Harley, Ross')){
#   
#   RD <- spatial_behaviour(data = iRB, recorder_name = recorder,
#                           latitude_col = 'lat', longitude_col = 'long',
#                           h = 5000)
#   plot_ratio(data = RD)
#   
# }

# Apply to all recorders
pdf(file = 'recorderAreas.pdf')
all_spatial <- lapply(unique(iRB$recorders), FUN = function(x){
  # cat(paste(x, '\n'))
  recorder_info <- spatial_behaviour(data = iRB, recorder_name = x,
                                     latitude_col = 'lat', longitude_col = 'long')
  if(!is.na(recorder_info$ratio)) plot_ratio(recorder_info)
  return(data.frame(recorder = recorder_info$recorder,
                    upper_area = recorder_info$upper_area,
                    lower_area = recorder_info$lower_area,
                    upper_n_poly = recorder_info$upper_n_poly,
                    lower_n_poly = recorder_info$lower_n_poly,
                    ratio = recorder_info$ratio,
                    n = recorder_info$n))
})
dev.off()

# combine results
temp <- do.call(rbind, all_spatial)
temp <- temp[temp$n > 400, ]

# Lets have a look at some people who have recorded a lot
temp[order(temp$ratio, decreasing = TRUE),]
```

Lets have a look at two people with very different ratios

```{r plot_ratios}
# Get the names of top and bottom
temp <- temp[order(temp$ratio, decreasing = TRUE),]
top <- as.character(head(temp$recorder, 1))
bottom <- as.character(tail(temp$recorder, 1))

# Plot the top and bottom ratio recorder
for(i in c(top, bottom)){
  top_d <- spatial_behaviour(data = iRB,
                             recorder_name = i,
                             latitude_col = 'lat',
                             longitude_col = 'long')
  plot_ratio(data = top_d)
}
```


## Taxanomic Metrics

These metric relate the the species that people record

### Taxanomic Breadth

This is simply a measure of the proportion of taxa a person has recorded. Note this is going to be correlated to the number of records.

```{r taxa_breadth}
taxa_breadth <- function(data, recorder_name,
                         sp_col = 'preferred_taxon',
                         recorder_col = 'recorders'){
  
  data_rec <- data[data[,recorder_col] == recorder_name, c(sp_col, recorder_col)]
  
  return(data.frame(recorder = recorder_name,
                    taxa_breadth = length(unique(data_rec[ ,sp_col])),
                    taxa_prop = length(unique(data_rec[ ,sp_col]))/length(unique(data[ ,sp_col])),
                    n = nrow(data_rec)))
}

taxa_breadth <- do.call(rbind, lapply(unique(iRB$recorders), FUN = taxa_breadth, data = iRB))

temp <- taxa_breadth[taxa_breadth$n > 400, ]

# Lets have a look at some people who have recorded a lot
temp[order(temp$taxa_prop, decreasing = TRUE),]
```

### Species Rarity

We want to capture the rarity of the species that people record. For example are they just recording the common species or are they only recording the rare ones, or perhaps they are recording everything. Since we don't know the real frequency distribution we can only compare people to the global average in the dataset. We can look to see what the distribution of species rank for each recorder is and how this compares to all records. A recorder only interested in rare species will have a median rank higher than the average. A recorder only recording common species will have a value lower than the average.


```{r species_rank}
# Lets look at a recorder
species_rank <- function(data, recorder_name,
                         sp_col = 'preferred_taxon',
                         recorder_col = 'recorders'){
  
  data <- data[,c(sp_col, recorder_col)]
  rank_species <- rank(abs(table(data[,sp_col])-max(table(data[,sp_col]))))
  sp_counts <- table(data[,sp_col])
  
  rank_reps <- rep(rank_species, sp_counts)
  grand_median <- median(rank_reps)
  grand_sd <- sd(rank_reps)
  
  recorder_data <- data[data[,recorder_col] == recorder_name,]
  recorder_data$rank <- rank_species[recorder_data[ ,sp_col]]
  
  return(data.frame(recorder = as.character(recorder_name),
                    median = median(recorder_data$rank),
                    median_diff = median(recorder_data$rank) - grand_median,
                    stdev = sd(recorder_data$rank),
                    n = nrow(recorder_data)))
}

rarity_preference <- do.call(rbind,
                             lapply(unique(iRB$recorders),
                                    FUN = species_rank,
                                    data = iRB))

temp <- rarity_preference[rarity_preference$n > 400, ]

# Lets have a look at some people who have recorded a lot
temp[order(temp$median_diff, decreasing = TRUE),]

```

Here `median_diff` gives the difference between the grand median for all records and the recorders median. This suggests `Saville, Simon` prefers to record rare species and `Cornish, Stephen` prefers to record common species.

This could be correlated to the number of records. 

```{r rarity_nrec}
mod <- glm(median ~ log(n), data = rarity_preference, family = 'quasipoisson')
summary(mod)
plot(log(rarity_preference$n),
     rarity_preference$median,
     xlab = 'log(N)',
     ylab = 'Median rarity')
```

There is a significant negative relationship. The more records you make the lower your median value. This could be a result of the fact that people who make only a few records record rare stuff?

```{r rarity_nrec_above}
rarity_preference_above <- rarity_preference[rarity_preference$n > 5, ]
mod <- glm(median ~ log(n), data = rarity_preference_above, family = 'quasipoisson')
summary(mod)
plot(log(rarity_preference_above$n),
     rarity_preference_above$median,
     xlab = 'log(N)',
     ylab = 'Median rarity')
```

Okay, the relationship falls down once we get rid of the people who only record a few species. I suggest this metric not be estimates for people who contribute only a few records. The relationship might actually be between deviation from the median and `n`.

```{r rarity_nrec_deviation}
rarity_preference$median_diff_abs <- abs(rarity_preference$median_diff)
mod <- glm(median_diff_abs ~ log(n), data = rarity_preference, family = 'quasipoisson')
summary(mod)
plot(log(rarity_preference$n),
     rarity_preference$median_diff_abs,
     xlab = 'log(N)',
     ylab = 'Difference in rarity')

plot(log(rarity_preference$n[rarity_preference$n >5]),
     rarity_preference$median_diff_abs[rarity_preference$n >5],
     xlab = 'log(N)',
     ylab = 'Difference in rarity')
```

The more records you record the less you deviate from the median. This is probably because you only get extreme values where the sample size is small.

### Species accumulation curve

The rate at which a recorder accumulates species over time recorded could be seen as a measure of their effort and expertise. The time scale will be measured in days recorded (not days elapsed). This measure will need to be normalised within a taxonomic group as clearly one can accumulate species more rapidly for birds than amphibians and reptiles. Note that while we look only at active days we have no way of accounting for actual time (hrs) recorded as a 10 minute search on one days appears the same as a 3 hour search on another. This metric is therefore an approximation only.

```{r species_accumulation}
# This can be run with all data
species_accumulation <- function(data, recorder_name,
                                 n_taxa, prediction_days = c(10, 50),
                                 sp_col = 'preferred_taxon',
                                 date_col = 'date_start',
                                 recorder_col = 'recorders',
                                 plot = FALSE, ...){

  # Get the data for this recorder
  rec_data <- data[data[,recorder_col] == recorder_name,
                   c(date_col, sp_col)]

  # sort dates
  dates <- sort(unique(rec_data$date_start))
  
  # Only carry out this test on people with more than the 
  # required number of days in their data
  if(length(dates) >= max(prediction_days)){
    
    # accumulation function
    acc <- function(x){
      length(unique(rec_data[,sp_col][rec_data[,date_col] <= x]))
    }
    
    species_accumulation_data <- sapply(dates, FUN = acc)
    day <- seq_along(species_accumulation_data)
    
    # Fit a model
    m <- glm(formula = species_accumulation_data ~ day + sqrt(day))
    m_sum <- summary(m)
    # predict atleast up to day 100 (could be dangerous)
    # but just for visualiation purposes
    days_to_predict <- ifelse(test = length(dates) > max(prediction_days),
                              yes = length(dates),
                              no = prediction_days)
    predicted <- predict(m, newdata = data.frame(day = 1:days_to_predict))
    
    if(plot){
      plot(species_accumulation_data, 
           main = paste('Species accumulation - ', recorder_name),
           xlab = 'days',
           ylab = 'Number of species',
           col = 'grey40',
           pch = 20, 
           ... = ...)
      lines(predicted, col = 'red', lwd = 3, lty = 5)
      for(pred_day in prediction_days){
        lines(x = rep(pred_day, 2), y = c(0, predicted[pred_day]),
        lty = 3, col = 'grey20', lwd = 2)
        text(x = pred_day, y = predicted[pred_day] + 5, cex = 1.5, 
           labels = round(predicted[pred_day]/n_taxa, 2))
      }
    }  
    
    # create named vector for predictions
    x <- predicted[prediction_days]/n_taxa
    names(x) <- paste0('d', prediction_days)
    x <- data.frame(t(x))
    x$recorder <- as.character(recorder_name)
    
    return(list(species_accumulation_data = species_accumulation_data,
                predicted_data = predicted,
                day_pred = x,
                n_day = length(dates)))
      
  } else {
    
    # empty df
    eDF <- as.data.frame(cbind(matrix(data = rep(NA,
                                          length(prediction_days)),
                               nrow = 1,
                               dimnames = list(recorder_name, paste0('d', prediction_days)))))
    
    eDF$recorder <- as.character(recorder_name)
    
    return(list(species_acculation_data = NA,
                predicted_data = NA,
                day_pred = eDF,
                n_day = length(dates)))
    
  }
}

# Run for a few people
par(mfrow = c(3, 2))

# top recorders
user_dates <- tapply(iRB$date_start, iRB$recorders, FUN = function(x) length(unique(x)))
recorders <- sample(names(sort(user_dates[user_dates > 50], decreasing = TRUE)), size = 6)

for(recorder in recorders){
user_acc <- species_accumulation(data = iRB, n_taxa = length(unique(iRB$preferred_taxon)),
                                 recorder_name = recorder,
                                 plot = TRUE,
                                 prediction_days = seq(10, 50, 10),
                                 # xlim = c(0, 100),
                                 ylim = c(0, length(unique(iRB$preferred_taxon))))
}

par(mfrow = c(1, 1))

# Calculate for everyone
all_acc_list <- lapply(X = unique(iRB$recorders), FUN = function(x){
  day_pred <- species_accumulation(recorder_name = x,
                                   data = iRB,
                                   n_taxa = length(unique(iRB$preferred_taxon)),
                                   plot = FALSE,
                                   prediction_days = seq(10, 50, 10))$day_pred
})

all_acc <- do.call(rbind, all_acc_list)

# These values clearly rise over time
boxplot(na.omit(all_acc[,!colnames(all_acc) %in% 'recorder']),
        xlab = 'Day',
        ylab = 'Proportion of taxa recorded',
        main = 'Species accumulation over time')

# How do these correlate?
cor_data <- cor.matrix(na.omit(all_acc[,!colnames(all_acc) %in% 'recorder']))
ggcorplot(cor.mat = cor_data,
          data = na.omit(all_acc[,!colnames(all_acc) %in% 'recorder']),
          var_text_size = 5,
          cor_text_limits = c(5,10))
```